---
title: Machine Learning Algorithm
---

### Simple Linear Regression

Simple linear regression is a basic predictive modeling technique that models the relationship between one input variable (X) and one output variable (Y).

#### How it Works

1. **The Line Equation**
   ```
   Y = mX + b
   ```
   - Y: Predicted value (dependent variable)
   - X: Input value (independent variable) 
   - m: Slope (how much Y changes when X changes)
   - b: Y-intercept (value of Y when X = 0)

2. **Finding Best Fit**
   - Uses "least squares" method
   - Minimizes the sum of squared differences between predicted and actual Y values
   - Lower error = better fit

#### Example

Predicting house prices based on square footage:
- X = Square footage (input)
- Y = House price (prediction)
- m = Price increase per square foot
- b = Base price

#### When to Use

- One input variable, one output variable
- Data shows roughly linear pattern
- Quick insights needed
- Basic predictions

#### Limitations

- Only handles linear relationships
- Sensitive to outliers
- Too simple for complex problems

#### Code Example
```python
# Basic implementation using sklearn
from sklearn.linear_model import LinearRegression
import numpy as np

X = np.array([[1], [2], [3], [4]])  # Input data
y = np.array([2, 4, 6, 8])          # Output data

model = LinearRegression()
model.fit(X, y)

# Predict new value
prediction = model.predict([[5]])
```

#### Real World Example: House Price Prediction

Let's predict house prices using square footage:

```python
import pandas as pd
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt

# Sample data
house_data = {
    'sqft': [1200, 1500, 1800, 2200, 2500],
    'price': [150000, 175000, 210000, 250000, 290000]
}
df = pd.DataFrame(house_data)

# Prepare data
X = df[['sqft']].values
y = df['price'].values

# Train model
model = LinearRegression()
model.fit(X, y)

# Get equation components
slope = model.coef_[0]
intercept = model.intercept_

print(f"Price = {slope:.2f} × sqft + {intercept:.2f}")

# Predict price for a 2000 sqft house
new_house = [[2000]]
predicted_price = model.predict(new_house)
print(f"Predicted price for 2000 sqft: ${predicted_price[0]:,.2f}")
```

**What This Shows:**
- Each square foot increases price by a fixed amount (slope)
- Base price is the intercept
- Model learns from existing house prices
- Can predict prices for new houses

**Output Example:**
```
Price = 110.23 × sqft + 15000.00
Predicted price for 2000 sqft: $235,460.00
```
### Cost Function

The cost function helps us measure how well our linear regression line fits the data. Think of it as a "wrongness score" - the lower the score, the better the fit.

#### How it Works

1. **Mean Squared Error (MSE)**
   ```
   MSE = (1/n) * Σ(y_actual - y_predicted)²
   ```
   - n: Number of data points
   - y_actual: Real value
   - y_predicted: Model's prediction
   - Σ: Sum everything

2. **Why Square the Errors?**
   - Makes all errors positive
   - Penalizes big mistakes more
   - Easier to calculate the minimum

#### Visual Example
```python
import numpy as np
import matplotlib.pyplot as plt

# Sample data
X = np.array([1, 2, 3, 4, 5])
y_actual = np.array([2, 4, 5, 4, 5])

# Bad fit line
m_bad = 0.5
b_bad = 1
y_bad = m_bad * X + b_bad

# Good fit line
m_good = 0.8
b_good = 1.5
y_good = m_good * X + b_good

# Calculate MSE
mse_bad = np.mean((y_actual - y_bad)**2)
mse_good = np.mean((y_actual - y_good)**2)

print(f"Bad fit MSE: {mse_bad:.2f}")
print(f"Good fit MSE: {mse_good:.2f}")
```

#### Finding the Best Line
1. Start with random slope (m) and intercept (b)
2. Calculate MSE
3. Adjust m and b to reduce MSE
4. Repeat until MSE can't get lower

#### Code Example
```python
from sklearn.metrics import mean_squared_error

# Sample data
X = np.array([[1], [2], [3], [4]])
y_true = np.array([2, 4, 6, 8])

# Train model
model = LinearRegression()
model.fit(X, y_true)

# Make predictions
y_pred = model.predict(X)

# Calculate cost
mse = mean_squared_error(y_true, y_pred)
print(f"Model's MSE: {mse:.2f}")
```

#### Key Points
- Lower cost = better fit
- Perfect fit has cost of 0
- Used to train the model
- Helps prevent overfitting

### Convergence Algorithm

Gradient descent helps find the best line by gradually adjusting the slope and intercept. Think of it like walking downhill to find the lowest point.

#### How it Works

1. **Basic Steps**
   ```
   For each step:
   1. Calculate current error
   2. Find direction of steepest descent
   3. Take a small step in that direction
   4. Repeat until minimal improvement
   ```

2. **Learning Rate (α)**
   - Controls step size
   - Too large: might overshoot
   - Too small: takes too long
   - Typical values: 0.01 to 0.1

#### Simple Implementation
```python
import numpy as np

def gradient_descent(X, y, learning_rate=0.01, epochs=1000):
    m = 0  # Initial slope
    b = 0  # Initial intercept
    n = len(X)  # Number of data points
    
    for _ in range(epochs):
        # Current predictions
        y_pred = m * X + b
        
        # Calculate gradients
        dm = (-2/n) * sum(X * (y - y_pred))
        db = (-2/n) * sum(y - y_pred)
        
        # Update parameters
        m = m - learning_rate * dm
        b = b - learning_rate * db
        
    return m, b

# Example usage
X = np.array([1, 2, 3, 4, 5])
y = np.array([2, 4, 6, 8, 10])

final_m, final_b = gradient_descent(X, y)
print(f"Final equation: y = {final_m:.2f}x + {final_b:.2f}")
```

#### Convergence Types

1. **Batch Gradient Descent**
   - Uses all data points
   - More stable
   - Slower for large datasets

2. **Stochastic Gradient Descent**
   - Uses one random point
   - Faster but noisier
   - Better for large datasets

#### Stopping Conditions
- Maximum iterations reached
- Error change is very small
- Gradient becomes very small

#### Common Issues and Solutions

1. **Not Converging**
   - Reduce learning rate
   - Normalize input data
   - Check for data issues

2. **Slow Convergence**
   - Increase learning rate
   - Use momentum
   - Try different initialization

#### Code with Early Stopping
```python
def gradient_descent_with_stopping(X, y, learning_rate=0.01, 
                                 tolerance=1e-6, max_epochs=1000):
    m = b = 0
    prev_cost = float('inf')
    
    for epoch in range(max_epochs):
        y_pred = m * X + b
        cost = np.mean((y - y_pred) ** 2)
        
        # Check for convergence
        if abs(prev_cost - cost) < tolerance:
            print(f"Converged at epoch {epoch}")
            break
            
        # Update parameters
        dm = (-2/len(X)) * sum(X * (y - y_pred))
        db = (-2/len(X)) * sum(y - y_pred)
        
        m -= learning_rate * dm
        b -= learning_rate * db
        prev_cost = cost
        
    return m, b
```

#### Key Points
- Automatically finds best parameters
- Learning rate is crucial
- May need multiple runs
- Works for many ML algorithms

### Multiple Linear Regression

Multiple linear regression predicts an outcome using two or more input variables. Think of it as simple linear regression with more features.

#### How it Works

1. **The Equation**
   ```
   Y = b + m₁X₁ + m₂X₂ + ... + mₙXₙ
   ```
   - Y: Predicted value
   - b: Base value (intercept)
   - m₁, m₂, etc.: Coefficients for each feature
   - X₁, X₂, etc.: Input features

#### Real World Example: House Price Prediction

```python
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

# Sample data
house_data = {
    'sqft': [1200, 1500, 1800, 2200, 2500],
    'bedrooms': [2, 3, 3, 4, 4],
    'age': [5, 10, 15, 5, 8],
    'price': [150000, 175000, 210000, 250000, 290000]
}
df = pd.DataFrame(house_data)

# Prepare features and target
X = df[['sqft', 'bedrooms', 'age']]
y = df['price']

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Train model
model = LinearRegression()
model.fit(X_train, y_train)

# Show coefficients
for feature, coef in zip(X.columns, model.coef_):
    print(f"{feature}: ${coef:,.2f} impact")
print(f"Base price: ${model.intercept_:,.2f}")

# Predict new house
new_house = [[2000, 3, 10]]  # 2000 sqft, 3 beds, 10 years old
prediction = model.predict(new_house)
print(f"\nPredicted price: ${prediction[0]:,.2f}")
```

#### Feature Selection
Good features are:
- Related to what you're predicting
- Independent from each other
- Actually available in real use

#### Data Preparation
1. **Handle Missing Values**
   ```python
   # Fill missing values
   df.fillna(df.mean(), inplace=True)
   ```

2. **Scale Features**
   ```python
   from sklearn.preprocessing import StandardScaler
   
   scaler = StandardScaler()
   X_scaled = scaler.fit_transform(X)
   ```

#### Model Evaluation
```python
from sklearn.metrics import r2_score, mean_squared_error
import numpy as np

# Make predictions
y_pred = model.predict(X_test)

# Calculate metrics
r2 = r2_score(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))

print(f"R² Score: {r2:.2f}")
print(f"RMSE: ${rmse:,.2f}")
```

#### Key Points
- More features = more complex model
- Features should be meaningful
- Watch for multicollinearity
- Scale features if needed
- Check model assumptions

#### Limitations
- Assumes linear relationships
- Sensitive to outliers
- Can overfit with too many features
- Features should be independent

### Performance Metrics

Performance metrics help us understand how well our model is performing. Here are the key metrics for regression models.

#### Common Metrics

1. **Mean Squared Error (MSE)**
   ```python
   from sklearn.metrics import mean_squared_error
   
   mse = mean_squared_error(y_true, y_pred)
   ```
   - Measures average squared difference between predictions and actual values
   - Penalizes larger errors more
   - Always positive, lower is better

2. **Root Mean Squared Error (RMSE)**
   ```python
   rmse = np.sqrt(mean_squared_error(y_true, y_pred))
   ```
   - Square root of MSE
   - Same units as target variable
   - Easier to interpret than MSE

3. **R-squared (R²)**
   ```python
   from sklearn.metrics import r2_score
   
   r2 = r2_score(y_true, y_pred)
   ```
   - Shows percentage of variance explained
   - Range: 0 to 1 (higher is better)
   - 0.7 means model explains 70% of variance

#### Complete Example
```python
import numpy as np
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

def evaluate_model(y_true, y_pred):
    # Calculate metrics
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_true, y_pred)
    mae = mean_absolute_error(y_true, y_pred)
    
    # Print results
    print(f"MSE: {mse:.2f}")
    print(f"RMSE: {rmse:.2f}")
    print(f"R²: {r2:.2f}")
    print(f"MAE: {mae:.2f}")
    
    return mse, rmse, r2, mae

# Example usage
y_true = np.array([10, 20, 30, 40, 50])
y_pred = np.array([12, 18, 31, 38, 51])

evaluate_model(y_true, y_pred)
```

#### Cross-Validation
```python
from sklearn.model_selection import cross_val_score

def cv_evaluate(model, X, y, cv=5):
    # Get cross-validation scores
    scores = cross_val_score(model, X, y, cv=cv)
    
    print(f"CV Scores: {scores}")
    print(f"Mean Score: {scores.mean():.2f}")
    print(f"Std Dev: {scores.std():.2f}")
```

#### Visualization
```python
import matplotlib.pyplot as plt

def plot_predictions(y_true, y_pred):
    plt.scatter(y_true, y_pred)
    plt.plot([y_true.min(), y_true.max()], 
             [y_true.min(), y_true.max()], 
             'r--', lw=2)
    plt.xlabel('Actual Values')
    plt.ylabel('Predictions')
    plt.title('Actual vs Predicted')
    plt.show()
```

#### When to Use Each Metric

1. **Use RMSE when:**
   - You need error in same units as target
   - Large errors are particularly bad

2. **Use R² when:**
   - Explaining model to non-technical people
   - Comparing different models

3. **Use Cross-validation when:**
   - Dataset is small
   - Need reliable performance estimate

#### Key Points
- Use multiple metrics
- Consider your audience
- Check for overfitting
- Validate on test data
- Compare to baseline

### MSE, MAE and RMSE

These are the three most important error metrics for regression models. Let's understand each one simply.

#### Mean Absolute Error (MAE)
```
MAE = (1/n) * Σ|y_true - y_pred|
```

**What it means:**
- Average of absolute differences between predictions and actual values
- Easier to understand
- All errors weighted equally
- Same unit as your data

```python
from sklearn.metrics import mean_absolute_error

# Example
y_true = [10, 20, 30]
y_pred = [12, 18, 35]

mae = mean_absolute_error(y_true, y_pred)
print(f"MAE: {mae}")  # Shows average error in original units
```

#### Mean Squared Error (MSE)
```
MSE = (1/n) * Σ(y_true - y_pred)²
```

**What it means:**
- Square the errors before averaging
- Penalizes large errors more
- Units are squared (if predicting dollars, MSE is in dollars²)
- Always positive

```python
from sklearn.metrics import mean_squared_error

mse = mean_squared_error(y_true, y_pred)
print(f"MSE: {mse}")
```

#### Root Mean Square Error (RMSE)
```
RMSE = √MSE
```

**What it means:**
- Square root of MSE
- Back to original units
- Still penalizes large errors
- Most commonly used metric

```python
rmse = np.sqrt(mean_squared_error(y_true, y_pred))
print(f"RMSE: {rmse}")
```

#### Complete Example
```python
import numpy as np
from sklearn.metrics import mean_absolute_error, mean_squared_error

def compare_metrics(y_true, y_pred):
    # Calculate all metrics
    mae = mean_absolute_error(y_true, y_pred)
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    
    print("Example predictions vs actual:")
    for t, p in zip(y_true, y_pred):
        print(f"Actual: {t}, Predicted: {p}, Difference: {abs(t-p)}")
    
    print(f"\nMAE: {mae:.2f}")
    print(f"MSE: {mse:.2f}")
    print(f"RMSE: {rmse:.2f}")

# Test with house prices (in thousands)
actual = [200, 300, 400, 500]
predicted = [180, 320, 390, 510]

compare_metrics(actual, predicted)
```

#### When to Use Each

**Use MAE when:**
- You need simple interpretation
- All errors equally important
- Outliers are not a big concern

**Use MSE when:**
- Large errors are more important
- You're training models
- You don't need interpretable units

**Use RMSE when:**
- You want interpretable units
- Large errors matter more
- Comparing different models

#### Key Points
- MAE is most interpretable
- RMSE is most popular
- MSE is best for training
- Always use same metric when comparing models

### OVERFITING AND UNDERFITING

Understanding when your model learns too much or too little from the data.

#### What Are They?

1. **Underfitting**
   - Model is too simple
   - Doesn't capture important patterns
   - Poor performance on both training and test data
   - Like memorizing only basic rules

2. **Overfitting**
   - Model is too complex
   - Learns noise in training data
   - Great on training data, poor on test data
   - Like memorizing answers instead of understanding

#### Visual Example
```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures

# Generate sample data
X = np.linspace(0, 10, 100).reshape(-1, 1)
y = 3*X + np.sin(X)*2 + np.random.normal(0, 1.5, (100,1))

# Three models
def plot_fits():
    # Underfit: straight line
    underfit = LinearRegression()
    underfit.fit(X, y)
    y_under = underfit.predict(X)
    
    # Good fit: polynomial degree 3
    good = PolynomialFeatures(degree=3)
    X_good = good.fit_transform(X)
    model_good = LinearRegression().fit(X_good, y)
    y_good = model_good.predict(X_good)
    
    # Overfit: polynomial degree 15
    overfit = PolynomialFeatures(degree=15)
    X_over = overfit.fit_transform(X)
    model_over = LinearRegression().fit(X_over, y)
    y_over = model_over.predict(X_over)
    
    # Plot
    plt.scatter(X, y, color='gray', alpha=0.5, label='Data')
    plt.plot(X, y_under, 'r-', label='Underfit')
    plt.plot(X, y_good, 'g-', label='Good fit')
    plt.plot(X, y_over, 'b-', label='Overfit')
    plt.legend()
    plt.show()

plot_fits()
```

#### How to Detect

1. **Underfitting Signs:**
   - High training error
   - High validation error
   - Model makes very simple predictions

2. **Overfitting Signs:**
   - Low training error
   - High validation error
   - Model makes complex, wiggly predictions

#### Solutions

**For Underfitting:**
```python
# Add more features
from sklearn.preprocessing import PolynomialFeatures

# Create more complex features
poly = PolynomialFeatures(degree=2)
X_more_features = poly.fit_transform(X)

# Try more complex model
from sklearn.ensemble import RandomForestRegressor
complex_model = RandomForestRegressor(n_estimators=100)
```

**For Overfitting:**
```python
# Add regularization
from sklearn.linear_model import Ridge, Lasso

# L2 regularization
ridge = Ridge(alpha=1.0)
ridge.fit(X, y)

# L1 regularization
lasso = Lasso(alpha=1.0)
lasso.fit(X, y)

# Use cross-validation
from sklearn.model_selection import cross_val_score
scores = cross_val_score(model, X, y, cv=5)
```

#### Prevention Techniques

1. **Cross Validation**
```python
from sklearn.model_selection import train_test_split

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Train and evaluate
model.fit(X_train, y_train)
train_score = model.score(X_train, y_train)
test_score = model.score(X_test, y_test)

print(f"Training score: {train_score:.2f}")
print(f"Testing score: {test_score:.2f}")
```

2. **Learning Curves**
```python
from sklearn.model_selection import learning_curve

def plot_learning_curve(model, X, y):
    train_sizes, train_scores, val_scores = learning_curve(
        model, X, y, cv=5, n_jobs=-1, 
        train_sizes=np.linspace(0.1, 1.0, 10))
    
    plt.plot(train_sizes, train_scores.mean(axis=1), label='Training score')
    plt.plot(train_sizes, val_scores.mean(axis=1), label='Cross-validation score')
    plt.xlabel('Training examples')
    plt.ylabel('Score')
    plt.legend()
    plt.show()
```

#### Key Points
- Balance is crucial
- Use validation data
- Start simple, add complexity slowly
- Monitor training vs validation performance
- Use regularization when needed

### Linear Regression with Ordinary Least Squares (OLS)

OLS is the most common method to find the best-fitting line in linear regression. It minimizes the sum of squared differences between predictions and actual values.

#### How OLS Works

1. **The Basic Idea**
   - Find line that minimizes squared errors
   - Squared errors = (actual - predicted)²
   - Has a mathematical solution (no iteration needed)

2. **The Formula**
   ```
   β = (X^T X)^(-1) X^T y
   ```
   Where:
   - β: Coefficients (slope and intercept)
   - X: Input features
   - y: Target values
   - ^T: Transpose
   - ^(-1): Matrix inverse

#### Simple Implementation
```python
import numpy as np

def simple_ols(X, y):
    # Add column of 1s for intercept
    X = np.column_stack([np.ones(len(X)), X])
    
    # Calculate coefficients
    beta = np.linalg.inv(X.T @ X) @ X.T @ y
    
    # Return intercept and slope
    return beta[0], beta[1]

# Example usage
X = np.array([1, 2, 3, 4, 5])
y = np.array([2, 4, 5, 4, 5])

intercept, slope = simple_ols(X, y)
print(f"y = {slope:.2f}x + {intercept:.2f}")
```

#### Using Statsmodels (More Detailed)
```python
import statsmodels.api as sm

def detailed_ols(X, y):
    # Add constant
    X = sm.add_constant(X)
    
    # Fit model
    model = sm.OLS(y, X).fit()
    
    # Print summary
    print(model.summary().tables[1])
    
    return model

# Example with house prices
X = np.array([1500, 1800, 2000, 2200, 2500])  # Square footage
y = np.array([150000, 180000, 210000, 220000, 250000])  # Prices

model = detailed_ols(X, y)
```

#### Using Scikit-learn (Simple)
```python
from sklearn.linear_model import LinearRegression

def sklearn_ols(X, y):
    # Reshape X if needed
    if X.ndim == 1:
        X = X.reshape(-1, 1)
        
    # Fit model
    model = LinearRegression()
    model.fit(X, y)
    
    print(f"Slope: {model.coef_[0]:.2f}")
    print(f"Intercept: {model.intercept_:.2f}")
    print(f"R² Score: {model.score(X, y):.2f}")
    
    return model

# Example usage
model = sklearn_ols(X, y)
```

#### Assumptions of OLS
1. **Linearity**
   - Relationship is actually linear
   - Check with scatter plots

2. **Independence**
   - Observations are independent
   - No time series patterns

3. **Normality**
   - Residuals are normally distributed
   - Check with histogram

4. **Equal Variance**
   - Spread of residuals is constant
   - Check with residual plot

#### Checking Assumptions
```python
def check_assumptions(model, X, y):
    # Get predictions and residuals
    y_pred = model.predict(X)
    residuals = y - y_pred
    
    # Plot residuals
    plt.figure(figsize=(10, 4))
    
    # Residual plot
    plt.subplot(121)
    plt.scatter(y_pred, residuals)
    plt.axhline(y=0, color='r', linestyle='--')
    plt.xlabel('Predicted')
    plt.ylabel('Residuals')
    
    # Histogram of residuals
    plt.subplot(122)
    plt.hist(residuals, bins=20)
    plt.xlabel('Residuals')
    plt.ylabel('Frequency')
    
    plt.tight_layout()
    plt.show()
```

#### Key Points
- Simple and fast
- Has exact solution
- Works well for linear data
- Check assumptions
- Use with small/medium datasets

### Linear Regression with Regularization

Regularization helps prevent overfitting by adding a penalty for large coefficients. Think of it as making the model simpler.

#### Types of Regularization

1. **Ridge (L2)**
   ```
   Cost = MSE + α * (sum of squared coefficients)
   ```
   - Shrinks coefficients toward zero
   - Never makes them exactly zero
   - Good for handling multicollinearity

2. **Lasso (L1)**
   ```
   Cost = MSE + α * (sum of absolute coefficients)
   ```
   - Can make coefficients exactly zero
   - Good for feature selection
   - Simpler models

#### Simple Example
```python
from sklearn.linear_model import Ridge, Lasso
import numpy as np

# Sample data
X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])
y = np.array([6, 8, 9, 11])

# Ridge regression
ridge = Ridge(alpha=1.0)
ridge.fit(X, y)
print("Ridge coefficients:", ridge.coef_)

# Lasso regression
lasso = Lasso(alpha=1.0)
lasso.fit(X, y)
print("Lasso coefficients:", lasso.coef_)
```

#### Real World Example: House Price Prediction
```python
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# Prepare data
house_data = {
    'sqft': [1200, 1500, 1800, 2200, 2500],
    'bedrooms': [2, 3, 3, 4, 4],
    'age': [5, 10, 15, 5, 8],
    'price': [150000, 175000, 210000, 250000, 290000]
}
df = pd.DataFrame(house_data)

# Scale features
X = df[['sqft', 'bedrooms', 'age']]
y = df['price']
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split data
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2)

# Try different alpha values
alphas = [0.1, 1.0, 10.0]
for alpha in alphas:
    # Ridge
    ridge = Ridge(alpha=alpha)
    ridge.fit(X_train, y_train)
    
    # Print coefficients
    print(f"\nRidge (alpha={alpha})")
    for name, coef in zip(X.columns, ridge.coef_):
        print(f"{name}: {coef:.2f}")
```

#### Finding Best Alpha
```python
from sklearn.model_selection import cross_val_score

def find_best_alpha(X, y, alphas):
    best_score = -float('inf')
    best_alpha = None
    
    for alpha in alphas:
        model = Ridge(alpha=alpha)
        scores = cross_val_score(model, X, y, cv=5)
        avg_score = scores.mean()
        
        if avg_score > best_score:
            best_score = avg_score
            best_alpha = alpha
            
    return best_alpha, best_score

# Example usage
alphas = [0.001, 0.01, 0.1, 1, 10, 100]
best_alpha, best_score = find_best_alpha(X_scaled, y, alphas)
print(f"Best alpha: {best_alpha}")
print(f"Best score: {best_score:.2f}")
```

#### When to Use Each

**Use Ridge when:**
- All features might be important
- Features are correlated
- Want to reduce coefficients

**Use Lasso when:**
- Need feature selection
- Want simpler model
- Some features might be useless

#### Elastic Net
```python
from sklearn.linear_model import ElasticNet

# Combines Ridge and Lasso
elastic = ElasticNet(alpha=1.0, l1_ratio=0.5)
elastic.fit(X_train, y_train)
```

#### Key Points
- Prevents overfitting
- Makes models more stable
- Scale features first
- Try different alpha values
- Use cross-validation

### Simple Polynomial Regression

Polynomial regression handles curved relationships by adding powers of X (like X², X³) to linear regression. Think of it as making linear regression flexible enough to fit curves.

#### How it Works

1. **Basic Idea**
   ```
   y = b + m₁x + m₂x² + m₃x³ + ...
   ```
   - b: Base value (intercept)
   - x: Input feature
   - x²,x³: Powers of x
   - m₁,m₂,m₃: Coefficients

#### Simple Implementation
```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
import numpy as np

def polynomial_regression(X, y, degree=2):
    # Convert X to polynomial features
    poly = PolynomialFeatures(degree=degree)
    X_poly = poly.fit_transform(X.reshape(-1, 1))
    
    # Fit model
    model = LinearRegression()
    model.fit(X_poly, y)
    
    return model, poly

# Example usage
X = np.array([1, 2, 3, 4, 5])
y = np.array([2, 4, 8, 16, 32])  # Exponential pattern

model, poly = polynomial_regression(X, y, degree=2)
```

#### Visual Example
```python
import matplotlib.pyplot as plt

def plot_polynomial_fit(X, y, degree):
    # Fit model
    model, poly = polynomial_regression(X, y, degree)
    
    # Generate smooth points for curve
    X_smooth = np.linspace(X.min(), X.max(), 100)
    X_smooth_poly = poly.transform(X_smooth.reshape(-1, 1))
    y_smooth = model.predict(X_smooth_poly)
    
    # Plot
    plt.scatter(X, y, color='blue', label='Data')
    plt.plot(X_smooth, y_smooth, color='red', label=f'Degree {degree}')
    plt.legend()
    plt.show()
    
    return model

# Example with different degrees
degrees = [1, 2, 3]
for degree in degrees:
    plot_polynomial_fit(X, y, degree)
```

#### Real World Example: Temperature Curve
```python
# Daily temperature data
hours = np.array([0, 4, 8, 12, 16, 20, 24])
temp = np.array([15, 13, 18, 25, 23, 18, 15])

def fit_temperature_curve():
    # Fit polynomial model
    model, poly = polynomial_regression(hours, temp, degree=3)
    
    # Generate smooth curve
    hours_smooth = np.linspace(0, 24, 100)
    hours_poly = poly.transform(hours_smooth.reshape(-1, 1))
    temp_smooth = model.predict(hours_poly)
    
    # Plot
    plt.scatter(hours, temp, label='Actual')
    plt.plot(hours_smooth, temp_smooth, 'r-', label='Predicted')
    plt.xlabel('Hour of Day')
    plt.ylabel('Temperature (°C)')
    plt.legend()
    plt.show()
```

#### Choosing the Right Degree

1. **Too Low (Underfitting)**
   - Line too rigid
   - Misses important patterns
   - High error on both training and test

2. **Too High (Overfitting)**
   - Line too wiggly
   - Fits noise in data
   - Perfect on training, bad on test

```python
def find_best_degree(X, y, max_degree=10):
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
    best_score = -float('inf')
    best_degree = 1
    
    for degree in range(1, max_degree + 1):
        model, poly = polynomial_regression(X_train, y_train, degree)
        
        # Transform test data
        X_test_poly = poly.transform(X_test.reshape(-1, 1))
        score = model.score(X_test_poly, y_test)
        
        if score > best_score:
            best_score = score
            best_degree = degree
    
    return best_degree, best_score
```

#### When to Use

**Good For:**
- Curved relationships
- Temperature cycles
- Growth patterns
- Physical processes

**Not Good For:**
- Linear relationships (use simple linear)
- Too many features
- Very noisy data

#### Key Points
- Start with low degrees (2 or 3)
- Check for overfitting
- Scale features if needed
- Use cross-validation
- Balance complexity vs accuracy

