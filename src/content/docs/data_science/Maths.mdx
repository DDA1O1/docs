---
title: Maths
---

## Statistics

Statistics is a branch of mathematics dealing with data collection, analysis, interpretation, and presentation. It provides tools for making informed decisions based on data.

### Key Concepts

- **Descriptive Statistics**: Summarizes data using measures like mean, median, mode, and standard deviation.
- **Inferential Statistics**: Makes predictions or inferences about a population based on a sample of data.

#### Population vs Sample

- **Population** ðŸ“Š
  - Complete dataset
  - Example: All students in a university
  - N = Total size

- **Sample** ðŸ”
  - Subset of population
  - Example: 100 randomly selected students
  - n = Sample size

> **Key Point**: Sample should be representative of the population


### Measures of Central Tendency

- Measures of Central Tendency are used to describe the central or typical value of a dataset.

- **Mean**: 
  - Average of all values in a dataset
  - Sample Formula: xÌ„ = (âˆ‘x) / n
  - Population Formula: Î¼ = (âˆ‘x) / N
  - Example: For [2,4,6,8], Mean = (2+4+6+8)/4 = 5
  - Use when:
    - Data is normally distributed
    - Need a value affected by all data points
  - Limitations:
    - Sensitive to outliers
    - May not represent central value in skewed data

- **Median**:
  - Middle value when data is ordered
  - Formula: 
    - Odd n: Value at position (n+1)/2
    - Even n: Average of values at n/2 and (n/2)+1
  - Example: For [1,3,5,7,9], Median = 5
  - Use when:
    - Data has outliers
    - Distribution is skewed
  - Limitations:
    - Not influenced by all values
    - Changes less smoothly than mean

- **Mode**:
  - Most frequently occurring value
  - Formula: Value with highest frequency
  - Example: For [1,2,2,3,4], Mode = 2
  - Use when:
    - Working with categorical data
    - Need most common value
  - Limitations:
    - Can have multiple modes
    - May not exist if all values occur once

### Measures of Dispersion

Measures of dispersion describe how spread out data points are from the center. They're crucial for:
- Understanding data variability
- Assessing data reliability
- Comparing datasets

#### Common Measures

- **Variance** (ÏƒÂ²)
  - Average squared deviation from mean
  - Formula: ÏƒÂ² = Î£(x - Î¼)Â²/n
  - Use when:
    - Detailed spread analysis needed
    - Computing statistical tests
  - Limitation: Units are squared

- **Standard Deviation** (Ïƒ)
  - Square root of variance
  - Formula: Ïƒ = âˆš(Î£(x - Î¼)Â²/n)
  - Use when:
    - Need spread in original units
    - Analyzing normal distributions
    - Building ML models

#### Variance

Variance measures the average squared distance of data points from their mean, indicating data spread and variability.

##### Key Points

- **Definition**: Average of squared deviations from mean
- **Population Formula**: ÏƒÂ² = Î£(x - Î¼)Â²/N
- **Sample Formula**: sÂ² = Î£(x - xÌ„)Â²/(n-1)

##### Why Use n-1 in Sample Variance?

The use of (n-1) instead of n in sample variance is called **Bessel's Correction**. Here's why it matters:

1. **Degrees of Freedom**
   - When calculating sample variance, we lose one degree of freedom
   - This happens because we already used one piece of information (sample mean)
   - n-1 accounts for this lost degree of freedom

2. **Bias Correction**
   - Sample variance with n tends to underestimate population variance
   - Using n-1 makes the estimator unbiased
   - Formula: sÂ² = Î£(x - xÌ„)Â²/(n-1)

3. **Practical Impact**
   - More noticeable in small samples
   - Example:
     - n=5: 20% difference
     - n=100: 1% difference
   - Critical for accurate statistical inference

##### Real-World Example of n-1 in Sample Variance

Imagine a battery manufacturing plant:

**Population Data:**
- All 1000 batteries produced in a day
- True population mean (Î¼) = 1.5V
- Population readings vary between 1.3V to 1.7V
- True population variance = 0.024VÂ²

**Sample Test:**
- We test only 5 batteries: [1.3V, 1.4V, 1.5V, 1.6V, 1.7V]
- Sample mean (xÌ„) = 1.5V

**Variance Calculations:**
```python
# Using n (biased)
Variance_n = Î£(x - xÌ„)Â²/5
= [(1.3-1.5)Â² + (1.4-1.5)Â² + (1.5-1.5)Â² + (1.6-1.5)Â² + (1.7-1.5)Â²]/5
= 0.02VÂ² # Underestimates true variance (0.024VÂ²)

# Using n-1 (unbiased)
Variance_n1 = Î£(x - xÌ„)Â²/4
= 0.025VÂ² # Closer to true variance (0.024VÂ²)
```

**Why This Matters:**
- Using n: 0.02VÂ² (off by 0.004VÂ²)
- Using n-1: 0.025VÂ² (off by 0.001VÂ²)
- n-1 gives estimate closer to true population variance

This example shows how n-1 provides a better estimate of the true population variance. Note that in real situations, we usually don't know the true population variance - that's why we need good estimation methods.

> **Key Point**: Use n-1 for sample variance to get an unbiased estimate of population variance

```python
# Sample of 100 people's weights
mean = 70kg
variance = 25kgÂ²  # Standard deviation â‰ˆ 5kg

# Practical Use
size_range = mean Â± (2 Ã— âˆšvariance)
# = 70 Â± 10kg
# = 60kg to 80kg

# Example with weight data
mean = 70kg
std_dev = 5kg

# Coverage ranges
1Ïƒ range = 70 Â± 5kg   = 65-75kg   (covers 68%)
2Ïƒ range = 70 Â± 10kg  = 60-80kg   (covers 95%)  # Most commonly used
3Ïƒ range = 70 Â± 15kg  = 55-85kg   (covers 99.7%)
```

### Variables

Variables are characteristics that can be measured or categorized. They come in different types:

#### 1. Qualitative (Categorical) Variables
- **Nominal**
  - Categories with no order
  - Example: Colors (red, blue), Gender (male, female)
  - Analysis: Mode, frequency

- **Ordinal**
  - Categories with order
  - Example: Education (high school, bachelor's, master's)
  - Analysis: Median, percentiles

#### 2. Quantitative (Numerical) Variables
- **Discrete**
  - Countable values
  - Example: Number of children, Test score
  - Analysis: Mean, standard deviation

- **Continuous**
  - Infinite possible values
  - Example: Height, Weight, Time
  - Analysis: Mean, standard deviation, correlation

#### Variable Relationships
- **Independent Variable (X)**
  - Manipulated/controlled variable
  - Example: Study hours

- **Dependent Variable (Y)**
  - Outcome variable
  - Example: Test score

> **Note**: Variable type determines which statistical methods to use


### Random Variables

A random variable is a function that assigns numerical values to outcomes of a random experiment.

#### Types of Random Variables

1. **Discrete Random Variables**
   - Takes countable/finite values
   - Examples:
     - Number of heads in coin flips
     - Count of defective items
   - Properties:
     - Probability Mass Function (PMF)
     - Cumulative Distribution Function (CDF)

2. **Continuous Random Variables**
   - Takes infinite possible values
   - Examples:
     - Height of a person
     - Time to complete a task
   - Properties:
     - Probability Density Function (PDF)
     - Cumulative Distribution Function (CDF)

### Histogram

A histogram is a graphical representation of the distribution of data. It shows the frequency of each data point in a dataset.

#### Key Points

- **Definition**: Bar chart showing frequency of data points
- **Purpose**: Visualize data distribution
- **Components**:
  - X-axis: Data range (bins)
  - Y-axis: Frequency (count or percentage)
  - Bars: Represents frequency of data in each bin

#### Examples

```python
import numpy as np
import matplotlib.pyplot as plt

data = [1, 2, 2, 3, 3, 3, 4, 4, 5]
plt.figure(figsize=(8, 4))
plt.hist(data, bins=5, edgecolor='black')
plt.title('Simple Histogram')
plt.xlabel('Values')
plt.ylabel('Frequency')
plt.show()
```

![Histogram Output](/images/histogram.png)

### Percentiles and Quartiles

Percentiles and quartiles are measures that divide a dataset into equal portions.

#### Percentiles
- Divides data into 100 equal parts
- Pth percentile: Value below which P% of observations fall
- Common uses:
  - 50th percentile = median
  - Used in standardized testing (e.g., "90th percentile")

#### Quartiles
- Divides data into 4 equal parts
- Q1 (25th percentile): First quartile
- Q2 (50th percentile): Median
- Q3 (75th percentile): Third quartile
- IQR (Interquartile Range) = Q3 - Q1

#### Example

```python
import numpy as np

data = [2, 4, 6, 8, 10, 12, 14, 16]

# Quartiles
Q1 = np.percentile(data, 25)  # = 5
Q2 = np.percentile(data, 50)  # = 9
Q3 = np.percentile(data, 75)  # = 13
IQR = Q3 - Q1                 # = 8

# Any percentile
p90 = np.percentile(data, 90) # 14.6
```

### 5 Number Summary

The 5 number summary is a quick way to describe the distribution of a dataset. It consists of the minimum, first quartile, median, third quartile, and maximum.

- Minimum: Smallest value in the dataset
- First Quartile (Q1): 25th percentile
- Median: 50th percentile
- Third Quartile (Q3): 75th percentile
- Maximum: Largest value in the dataset

#### Example

```python
import numpy as np

data = [2, 4, 6, 8, 10, 12, 14, 16]

# 5 Number Summary
summary = np.percentile(data, [0, 25, 50, 75, 100])
print(summary)  # [2.  5.  9.  13. 16.]
```
Outliers are values that fall outside the range of the 5 number summary.

- Lower Outlier: Below (Q1 - 1.5 * IQR)
- Upper Outlier: Above (Q3 + 1.5 * IQR)
- Interquartile Range (IQR) = Q3 - Q1

### Example

```python
import numpy as np

# Sample dataset
data = [1, 2, 2, 3, 4, 10, 20, 25, 30]

# Calculate 5 number summary
min_val = np.min(data)
q1 = np.percentile(data, 25)
median = np.percentile(data, 50)
q3 = np.percentile(data, 75)
max_val = np.max(data)

# Calculate IQR and outlier bounds
iqr = q3 - q1
lower_bound = q1 - 1.5 * iqr
upper_bound = q3 + 1.5 * iqr

# Find outliers
outliers = [x for x in data if x < lower_bound or x > upper_bound]

print(f"5 Number Summary:")
print(f"Min: {min_val}")
print(f"Q1: {q1}")
print(f"Median: {median}")
print(f"Q3: {q3}")
print(f"Max: {max_val}")
print(f"Outliers: {outliers}")
```
### Covariance and Correlation

#### Covariance

Covariance measures how two variables change together. It indicates the direction of the linear relationship between variables.

#### Formula
- Population Covariance: Ïƒxy = Î£((x - Î¼x)(y - Î¼y))/N
- Sample Covariance: sxy = Î£((x - xÌ„)(y - È³))/(n-1)

#### Interpretation
- Positive covariance: Variables tend to move in same direction
- Negative covariance: Variables tend to move in opposite directions
- Zero covariance: No linear relationship

#### Example

```python
import numpy as np

# Height (cm) and Weight (kg) data
height = np.array([170, 175, 160, 180, 165, 172])
weight = np.array([65, 70, 55, 80, 60, 68])

# Calculate means
height_mean = np.mean(height)
weight_mean = np.mean(weight)

# Calculate covariance manually
n = len(height)
covariance = sum((height - height_mean) * (weight - weight_mean)) / (n-1)

# Using NumPy
cov_matrix = np.cov(height, weight)
covariance_np = cov_matrix[0,1]

print(f"Manual Covariance: {covariance:.2f}")
print(f"NumPy Covariance: {covariance_np:.2f}")

# Output:
# Manual Covariance: 58.17
# NumPy Covariance: 58.17
```

#### Key Points:
1. Covariance range: -âˆž to +âˆž
2. Scale-dependent (affected by units)
3. Used in:
   - Principal Component Analysis
   - Portfolio optimization
   - Feature selection in ML

#### Limitations:
1. Not standardized (hard to compare)
2. Units are product of both variables

For standardized measurement, use correlation instead.

#### Correlation

Correlation measures the strength and direction of the linear relationship between two variables. Unlike covariance, it's standardized between -1 and +1.

#### Pearson Correlation Coefficient
The most common correlation measure is Pearson's r:

#### Formula
- Population Correlation: Ïxy = Î£((x - Î¼x)(y - Î¼y))/(ÏƒxÏƒy)
- Sample Correlation: r = Î£((x - xÌ„)(y - È³))/âˆš[Î£(x - xÌ„)Â²Î£(y - È³)Â²] = Cov(x,y)/(ÏƒxÏƒy)

#### Interpretation
- r = 1: Perfect positive correlation
- r = -1: Perfect negative correlation  
- r = 0: No linear correlation
- |r| > 0.7: Strong correlation
- 0.3 < |r| < 0.7: Moderate correlation
- |r| < 0.3: Weak correlation

#### Example

```python
import numpy as np
import matplotlib.pyplot as plt

# Sample data
x = np.array([1, 2, 3, 4, 5])
y1 = np.array([2, 4, 6, 8, 10])  # Perfect positive
y2 = np.array([10, 8, 6, 4, 2])  # Perfect negative
y3 = np.array([2, 5, 4, 5, 3])   # Weak correlation

def plot_correlation(x, y, title):
    plt.scatter(x, y)
    plt.title(f'{title} (r = {np.corrcoef(x,y)[0,1]:.2f})')
    plt.xlabel('X')
    plt.ylabel('Y')

# Create subplots
plt.figure(figsize=(15, 5))

plt.subplot(131)
plot_correlation(x, y1, "Perfect Positive")

plt.subplot(132)
plot_correlation(x, y2, "Perfect Negative")

plt.subplot(133)
plot_correlation(x, y3, "Weak")

plt.tight_layout()
plt.show()

# Calculate correlations
print(f"Positive correlation: {np.corrcoef(x,y1)[0,1]:.2f}") # 1.00
print(f"Negative correlation: {np.corrcoef(x,y2)[0,1]:.2f}") # -1.00
print(f"Weak correlation: {np.corrcoef(x,y3)[0,1]:.2f}") # 0.24
```
![Correlation](/images/Pearson_Correlation.png)

#### Key Properties
1. Scale-independent (standardized)
2. Always between -1 and +1
3. No units
4. Symmetric: corr(x,y) = corr(y,x)

#### Common Uses
- Feature selection in ML
- Financial portfolio analysis
- Scientific research
- Quality control

#### Limitations
1. Only measures linear relationships
2. Sensitive to outliers
3. Correlation â‰  causation
4. Requires numeric data

#### Real-World Example
```python
import pandas as pd

# Student data
data = {
    'study_hours': [2, 3, 3, 4, 4, 5, 5, 6],
    'test_score': [65, 70, 75, 80, 85, 85, 90, 95]
}

df = pd.DataFrame(data)

# Calculate correlation
correlation = df['study_hours'].corr(df['test_score'])

print(f"Correlation between study hours and test scores: {correlation:.2f}")
# Output: Correlation between study hours and test scores: 0.97
```

#### Important Notes
1. High correlation doesn't imply causation
2. Always visualize data - don't rely solely on correlation coefficient
3. Consider non-linear relationships
4. Check for outliers that might affect correlation

### Spearman Correlation

Spearman correlation measures monotonic relationships between variables (whether they move in the same direction, regardless of the rate of change).

#### Formula

1. First convert values to ranks
   - rank(x): Rank values of first variable 
   - rank(y): Rank values of second variable

2. Calculate differences
   d = rank(x) - rank(y)

3. Square the differences
   dÂ² = (rank(x) - rank(y))Â²

4. Sum all squared differences
   Î£dÂ² = sum of all dÂ²

5. Final formula:
   Ï = 1 - (6 * Î£dÂ²)/(n(nÂ² - 1))

Example with numbers:
x = [1,2,3]
y = [2,1,3]

ranks_x = [1,2,3]
ranks_y = [2,1,3]

d = [1-2, 2-1, 3-3] = [-1,1,0]
dÂ² = [1,1,0]
Î£dÂ² = 2
n = 3

Ï = 1 - (6 * 2)/(3(9-1))
  = 1 - 12/24
  = 0.5

#### Example

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# Online Store Example
# X: Product Price
# Y: Number of Sales
price = np.array([10, 20, 30, 40, 50, 60, 70, 80, 90, 100])
sales = np.array([100, 90, 80, 65, 45, 30, 20, 15, 12, 10])

# Calculate Correlations
pearson = stats.pearsonr(price, sales)[0]
spearman = stats.spearmanr(price, sales)[0]

# Plotting
plt.figure(figsize=(10, 6))
plt.scatter(price, sales)
plt.title('Price vs Sales: Non-linear Relationship')
plt.xlabel('Price ($)')
plt.ylabel('Number of Sales')

# Add correlation values
plt.text(20, 30, f'Pearson r: {pearson:.2f}')
plt.text(20, 20, f'Spearman r: {spearman:.2f}')
plt.grid(True)
plt.show()

print(f"Pearson Correlation: {pearson:.2f}")   # -0.89
print(f"Spearman Correlation: {spearman:.2f}") # -1.00
```
![Spearman Correlation](/images/Spearman_Correlation.png)

## Probability

Probability is a measure of the likelihood of an event occurring. It's a fundamental concept in statistics and data science.

#### Additive Rule

#### Mutually Exclusive Events
Events that cannot occur at the same time.

Formula:
P(A or B) = P(A) + P(B)

Example:
- Rolling a die:
  - P(getting 1 or 2) = P(1) + P(2) = 1/6 + 1/6 = 1/3
  - Events are mutually exclusive since you can't roll 1 and 2 simultaneously

#### Non-Mutually Exclusive Events 
Events that can occur at the same time.

Formula:
P(A or B) = P(A) + P(B) - P(A and B)

Example:
- Drawing a card:
  - P(getting King or Heart) = P(King) + P(Heart) - P(King of Heart)
  - = 4/52 + 13/52 - 1/52 = 16/52
  - Events overlap since King of Hearts is possible

### Multiplicative Rule

The multiplicative rule calculates probability of multiple events occurring together.

#### Independent Events
Events where occurrence of one doesn't affect the other.

Formula:
P(A and B) = P(A) Ã— P(B)

Example:
- Flipping a coin twice:
  - P(2 heads) = P(head1) Ã— P(head2) = 1/2 Ã— 1/2 = 1/4

#### Dependent Events
Events where occurrence of one affects the other.

Formula:
P(A and B) = P(A) Ã— P(B|A)

Example:
- Drawing 2 cards without replacement:
  - P(2 aces) = P(ace1) Ã— P(ace2|ace1) 
  - = 4/52 Ã— 3/51 = 1/221

### Relationship between PMF, PDF, and CDF

#### 1. PMF (Probability Mass Function)

A PMF describes the probability distribution of a discrete random variable.

#### Definition
- Maps each value of discrete random variable to its probability
- P(X = x) gives probability of X taking value x
- Sum of all probabilities must equal 1

#### Properties
- 0 â‰¤ P(X = x) â‰¤ 1
- âˆ‘P(X = x) = 1
- Only for discrete variables

#### Real-World Example: Dice Roll Game

```python
import numpy as np
import matplotlib.pyplot as plt

# Fair die PMF
outcomes = np.array([1, 2, 3, 4, 5, 6])
probabilities = np.array([1/6] * 6)

# Loaded die PMF (favors 6)
loaded_prob = np.array([0.1, 0.1, 0.1, 0.1, 0.1, 0.5])

# Plot
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.bar(outcomes, probabilities)
plt.title('Fair Die PMF')
plt.xlabel('Outcome')
plt.ylabel('Probability')

plt.subplot(1, 2, 2)
plt.bar(outcomes, loaded_prob)
plt.title('Loaded Die PMF')
plt.xlabel('Outcome')
plt.show()

# Calculate probability of rolling even numbers
fair_even = sum(probabilities[1::2])    # 0.5
loaded_even = sum(loaded_prob[1::2])    # 0.7

print(f"P(Even) Fair Die: {fair_even}") # 0.5
print(f"P(Even) Loaded Die: {loaded_even}") # 0.7
```
![PMF](/images/PMF.png)

PMF helps in making probability-based decisions in discrete scenarios like manufacturing defects, customer counts, or game outcomes.

#### CDF (Cumulative Distribution Function) For Discrete Variables

CDF gives the probability that a random variable X is less than or equal to a value x.

#### Formula
F(x) = P(X â‰¤ x) = âˆ‘ P(X = t) for all t â‰¤ x

#### Example: Die Roll

```python
import numpy as np
import matplotlib.pyplot as plt

# Define probabilities
outcomes = np.arange(1, 7)  # [1,2,3,4,5,6]
fair_prob = np.ones(6) / 6  # [1/6, 1/6, 1/6, 1/6, 1/6, 1/6]
loaded_prob = np.array([0.1, 0.1, 0.1, 0.1, 0.1, 0.5])

# Calculate CDFs
fair_cdf = np.cumsum(fair_prob)
loaded_cdf = np.cumsum(loaded_prob)

# Create plot
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))

# Plot fair die CDF
ax1.step(outcomes, fair_cdf, where='post')
ax1.set(title='Fair Die CDF', xlabel='Outcome', ylabel='Cumulative Probability')
ax1.grid(True)

# Plot loaded die CDF
ax2.step(outcomes, loaded_cdf, where='post')
ax2.set(title='Loaded Die CDF', xlabel='Outcome')
ax2.grid(True)

plt.tight_layout()
plt.show()

# Print probabilities for X â‰¤ 4
print(f"P(X â‰¤ 4) Fair Die: {fair_cdf[3]:.3f}") # 0.667
print(f"P(X â‰¤ 4) Loaded Die: {loaded_cdf[3]:.3f}") # 0.400
```
![Discrete CDF](/images/Discrete_CDF.png)

#### 2. PDF (Probability Density Function)

A PDF describes the probability distribution of a continuous random variable.

### Why PDF for Continuous Random Variables?

1. **Impossible to List All Values**
   - Continuous variables have infinite possible values
   - Can't assign individual probabilities like PMF

2. **Zero Individual Probability**
   - P(X = x) = 0 for any exact value
   - Example: P(height = exactly 170.000000...cm) = 0

3. **Range Probabilities**
   - PDF helps calculate probability over intervals
   - P(a â‰¤ X â‰¤ b) = âˆ«[a to b] f(x)dx
   - Example: P(170 â‰¤ height â‰¤ 175)

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm

# Height Distribution in Adult Population
mean_height = 170  # cm
std_dev = 10       # cm

# Create height range
heights = np.linspace(140, 200, 100)

# Calculate PDF using normal distribution
pdf = norm.pdf(heights, mean_height, std_dev)

# Plot
plt.figure(figsize=(10, 6))
plt.plot(heights, pdf)
plt.title('Height Distribution in Adult Population')
plt.xlabel('Height (cm)')
plt.ylabel('Probability Density')

# Calculate probabilities
# Probability of height between 160-180cm
prob_160_180 = norm.cdf(180, mean_height, std_dev) - norm.cdf(160, mean_height, std_dev)
print(f"Probability of height between 160-180cm: {prob_160_180:.2%}")  # â‰ˆ 68%

# Probability of height above 190cm
prob_above_190 = 1 - norm.cdf(190, mean_height, std_dev)
print(f"Probability of height above 190cm: {prob_above_190:.2%}")      # â‰ˆ 2.3%
```
![Continuous PDF](/images/Continuous_PDF.png)

### Density

Density in statistics refers to how tightly packed data points or probability is in a given interval or region.

```python
import numpy as np
from scipy.stats import norm
import matplotlib.pyplot as plt

mean = 170
std = 10

# Single point density doesn't tell much
point_density = norm.pdf(170, mean, std)
print(f"Density at 170cm: {point_density:.4f}")  # 0.0399
# This number 0.0399 alone is meaningless without context

# What makes sense is comparing densities
heights = [150, 160, 170, 180, 190]
densities = [norm.pdf(h, mean, std) for h in heights]

plt.figure(figsize=(10, 6))
x = np.linspace(140, 200, 1000)
y = norm.pdf(x, mean, std)

plt.plot(x, y)

# Plot points for comparison
for h, d in zip(heights, densities):
    plt.plot(h, d, 'o', label=f'Height={h}cm\nDensity={d:.4f}')

plt.title('Height Distribution - Comparing Densities')
plt.xlabel('Height (cm)')
plt.ylabel('Density')
plt.legend()
plt.grid(True)
plt.show()

# Now we can see:
# - 170cm has highest density (most common)
# - 150cm and 190cm have low density (less common)
# - Comparison gives meaning to the numbers
```
![Density](/images/Density.png)

#### CDF (Cumulative Distribution Function) For Continuous Variables

The Cumulative Distribution Function (CDF) for a continuous random variable X, denoted as F(x), represents the probability that X takes on a value less than or equal to x.

##### Mathematical Definition
F(x) = P(X â‰¤ x) = âˆ«[from -âˆž to x] f(t)dt

where f(t) is the probability density function (PDF)

##### Key Properties
1. **Bounds**
   - 0 â‰¤ F(x) â‰¤ 1 for all x
   - lim[xâ†’-âˆž] F(x) = 0
   - lim[xâ†’âˆž] F(x) = 1

2. **Continuity**
   - Right-continuous
   - Monotonically increasing (never decreases)

3. **Probability Calculations**
   - P(a < X â‰¤ b) = F(b) - F(a)
   - P(X > a) = 1 - F(a)

4. **Relationship to PDF**
   - F'(x) = f(x) (derivative of CDF is PDF)
   - F(x) is the integral of f(x)

##### Simple Example: Height Distribution in a Class

Consider heights of students in a class of 100:

**Scenario:**
- Heights range from 150cm to 190cm
- CDF tells us probability of height being less than or equal to a value

**Simple Interpretations:**
- F(160) = 0.2 means 20% of students are 160cm or shorter
- F(170) = 0.5 means 50% of students are 170cm or shorter
- F(180) = 0.9 means 90% of students are 180cm or shorter

**Practical Uses:**
- Finding median height: Where F(x) = 0.5
- Ordering uniforms: What size covers 80% of students
- Identifying unusually tall/short: Heights where F(x) < 0.1 or F(x) > 0.9

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm

# Parameters
mean_height = 170  # mean height in cm
std_dev = 10      # standard deviation in cm
n_students = 100

# Generate student heights
np.random.seed(42)  # for reproducibility
heights = np.random.normal(mean_height, std_dev, n_students)

# Calculate empirical CDF
heights_sorted = np.sort(heights)
cumulative_prob = np.arange(1, len(heights) + 1) / len(heights)

# Plot
plt.figure(figsize=(10, 6))

# Empirical CDF
plt.plot(heights_sorted, cumulative_prob, 'b-', label='Empirical CDF')

# Add reference lines
plt.axhline(y=0.2, color='r', linestyle='--', alpha=0.5)
plt.axhline(y=0.5, color='r', linestyle='--', alpha=0.5)
plt.axhline(y=0.9, color='r', linestyle='--', alpha=0.5)

# Labels and title
plt.title('Height Distribution CDF in Class')
plt.xlabel('Height (cm)')
plt.ylabel('Cumulative Probability')
plt.grid(True)

# Find specific values
height_20 = np.percentile(heights, 20)
height_50 = np.percentile(heights, 50)
height_90 = np.percentile(heights, 90)

print(f"20th percentile (F(x) = 0.2): {height_20:.1f}cm") # 162.6cm
print(f"50th percentile (F(x) = 0.5): {height_50:.1f}cm") # 168.7cm
print(f"90th percentile (F(x) = 0.9): {height_90:.1f}cm") # 180.1cm

plt.show()
```
![Continuous CDF](/images/Continuous_CDF.png)

### Types of Probability Distributions

#### Bernoulli Distribution

The Bernoulli distribution models binary outcomes - experiments with exactly two possible results (success/failure).

##### Properties
- **Parameter**: p (probability of success)
- **Possible Values**: x âˆˆ {0,1}
  - x = 1 (success): probability = p
  - x = 0 (failure): probability = 1-p
- **PMF**: P(X = x) = p^x * (1-p)^(1-x)
- **Mean**: E(X) = p
- **Variance**: Var(X) = p(1-p)

##### Common Applications
- Coin flips (heads/tails)
- Quality control (defective/non-defective)
- Email (spam/not spam)
- Medical tests (positive/negative)

##### Example Implementation

```python
import numpy as np
import matplotlib.pyplot as plt

class BernoulliTrial:
    def __init__(self, p):
        self.p = p
    
    def pmf(self, x):
        return self.p if x == 1 else (1-self.p)
    
    def simulate(self, n_trials):
        return np.random.binomial(n=1, p=self.p, size=n_trials)

# Example: Biased coin (p=0.7)
b = BernoulliTrial(p=0.7)
trials = b.simulate(1000)

# Results
success_rate = np.mean(trials)
print(f"Theoretical probability: 0.7")
print(f"Observed probability: {success_rate:.3f}")

# Visualize
plt.figure(figsize=(8, 4))
plt.bar(['Failure (0)', 'Success (1)'], 
        [1-success_rate, success_rate])
plt.title('Bernoulli Distribution (p=0.7)')
plt.ylabel('Probability')
plt.ylim(0, 1)
```
![Bernoulli Distribution](/images/Bernoulli_Distribution.png)

##### Real-World Example: Email Spam Detection

```python
# Simple spam detector
class SpamDetector:
    def __init__(self, spam_probability=0.3):
        self.b = BernoulliTrial(spam_probability)
    
    def classify_email(self):
        return "Spam" if self.b.simulate(1)[0] else "Not Spam"

# Simulate email classification
detector = SpamDetector()
n_emails = 100
classifications = [detector.classify_email() for _ in range(n_emails)]

spam_ratio = classifications.count("Spam") / n_emails
print(f"Classified {spam_ratio:.1%} emails as spam")
// Classified 26.0% emails as spam
```

##### Key Points
1. **Independence**: Each trial is independent
2. **Memory-less**: Previous outcomes don't affect next trial
3. **Fixed probability**: p remains constant across trials

##### Relationship to Other Distributions
- Foundation for Binomial distribution (n Bernoulli trials)
- Special case of Binomial where n=1
- Building block for more complex probability models

#### Binomial Distribution

The Binomial distribution models the number of successes in n independent Bernoulli trials.

##### Properties
- **Parameters**: 
  - n (number of trials)
  - p (probability of success)
- **PMF**: P(X = k) = C(n,k) * p^k * (1-p)^(n-k)
- **Mean**: E(X) = np
- **Variance**: Var(X) = np(1-p)

##### Example Implementation

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import binom

class BinomialDistribution:
    def __init__(self, n, p):
        self.n = n
        self.p = p
    
    def pmf(self, k):
        return binom.pmf(k, self.n, self.p)
    
    def simulate(self, n_trials):
        return np.random.binomial(self.n, self.p, n_trials)

# Example: Rolling a fair die 10 times, counting 6s
n, p = 10, 1/6  # 10 rolls, P(6) = 1/6
b = BinomialDistribution(n, p)

# Calculate PMF for all possible values
k = np.arange(0, n+1)
probabilities = [b.pmf(ki) for ki in k]

# Plot
plt.figure(figsize=(10, 5))
plt.bar(k, probabilities)
plt.title(f'Binomial Distribution (n={n}, p={p:.2f})')
plt.xlabel('Number of Successes (k)')
plt.ylabel('Probability')
plt.grid(True, alpha=0.3)

# Expected value and variance
mean = n * p
var = n * p * (1-p)
print(f"Expected number of 6s: {mean:.2f}") # 1.67
print(f"Variance: {var:.2f}") # 1.39
```
![Binomial Distribution](/images/Binomial_Distribution.png)

##### Real-World Example: Quality Control

```python
# Manufacturing defect inspection
class QualityControl:
    def __init__(self, batch_size=20, defect_rate=0.05):
        self.binom = BinomialDistribution(batch_size, defect_rate)
    
    def inspect_batch(self):
        return self.binom.simulate(1)[0]
    
    def is_batch_acceptable(self, max_defects=2):
        defects = self.inspect_batch()
        return {
            'defects': defects,
            'acceptable': defects <= max_defects
        }

# Simulate batch inspections
qc = QualityControl()
n_batches = 1000
inspections = [qc.is_batch_acceptable() for _ in range(n_batches)]

acceptance_rate = sum(i['acceptable'] for i in inspections) / n_batches
print(f"Batch acceptance rate: {acceptance_rate:.1%}")
// Batch acceptance rate: 92.7%
```

##### Common Applications
1. Quality control in manufacturing
2. A/B testing success counts
3. Survey response modeling
4. Genetic inheritance patterns

##### Key Points
1. Sum of independent Bernoulli trials
2. Requires fixed probability p
3. Trials must be independent
4. Only whole numbers (discrete)

#### Poisson Distribution

The Poisson distribution models the number of events occurring in a fixed interval when these events happen at a constant average rate and independently of each other.

##### Properties
- **Parameter**: Î» (lambda) - average number of events per interval
- **PMF**: P(X = k) = (Î»^k * e^-Î») / k!
- **Mean**: E(X) = Î»
- **Variance**: Var(X) = Î»

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import poisson

class PoissonDistribution:
    def __init__(self, lambda_param):
        self.lambda_param = lambda_param
    
    def pmf(self, k):
        return poisson.pmf(k, self.lambda_param)
    
    def simulate(self, n_samples):
        return np.random.poisson(self.lambda_param, n_samples)

# Example: Website visits per hour (average 5 visits)
lambda_param = 5
p = PoissonDistribution(lambda_param)

# Calculate PMF for values 0 to 12
k = np.arange(0, 13)
probabilities = [p.pmf(ki) for ki in k]

# Visualization
plt.figure(figsize=(10, 6))
plt.bar(k, probabilities)
plt.title(f'Poisson Distribution (Î»={lambda_param})')
plt.xlabel('Number of Events (k)')
plt.ylabel('Probability')
plt.grid(True, alpha=0.3)
```
![Poisson Distribution](/images/Poisson_Distribution.png)

##### Common Applications
1. **Customer Service**
   - Number of customers arriving per hour
   - Support tickets received per day
   - Phone calls to call center

2. **Web Traffic**
   - Page views per minute
   - Server requests per second
   - Error occurrences per day

3. **Quality Control**
   - Defects per unit area
   - Flaws per length of material
   - Errors per page

4. **Natural Phenomena**
   - Radioactive decay events
   - Mutations in DNA sequence
   - Natural disasters per year

##### Real-World Example: Server Monitoring

```python
class ServerMonitor:
    def __init__(self, avg_requests_per_minute=30):
        self.poisson = PoissonDistribution(avg_requests_per_minute)
    
    def simulate_minute(self):
        return self.poisson.simulate(1)[0]
    
    def check_load(self, threshold=50):
        requests = self.simulate_minute()
        return {
            'requests': requests,
            'overloaded': requests > threshold,
            'utilization': requests / threshold
        }

# Monitor server for an hour
monitor = ServerMonitor()
hour_data = [monitor.check_load() for _ in range(60)]

# Analysis
overloaded_minutes = sum(minute['overloaded'] for minute in hour_data)
avg_utilization = np.mean([minute['utilization'] for minute in hour_data])

print(f"Minutes overloaded: {overloaded_minutes}") // Minutes overloaded: 0
print(f"Average utilization: {avg_utilization:.1%}") // Average utilization: 56.5%
```

##### Key Characteristics

1. **Independence**
   - Events occur independently
   - Past events don't influence future events

2. **Rate Consistency**
   - Average rate (Î») remains constant
   - No systematic variation in event frequency

3. **Rare Events**
   - Individual events are rare relative to opportunities
   - Many opportunities for events to occur

4. **No Upper Limit**
   - Can theoretically take any non-negative integer value
   - Practical limits depend on Î»

##### Relationship to Other Distributions

1. **Binomial Distribution**
   - Poisson is limit of binomial as nâ†’âˆž, pâ†’0, np=Î»
   - Used when events are rare but opportunities numerous

2. **Exponential Distribution**
   - Time between Poisson events follows exponential distribution
   - If events are Poisson(Î»), waiting times are Exp(1/Î»)

##### Assumptions and Limitations

1. **Rate Stability**
   - Assumes constant average rate
   - May not fit if rate varies significantly

2. **Independence**
   - Events must be independent
   - Not suitable for contagious or clustered events

3. **No Simultaneous Events**
   - Events occur one at a time
   - May need modifications for concurrent events

4. **Memory-less Property**
   - Future events independent of past
   - May not suit events with temporal dependencies

```python
# Example: Testing Poisson assumptions
def test_rate_stability(data, window_size=10):
    """Test if event rate is stable over time"""
    windows = np.array_split(data, len(data)//window_size)
    means = [np.mean(w) for w in windows]
    return np.std(means) / np.mean(means)  # CV should be small

# Generate sample data
p = PoissonDistribution(lambda_param=5)
data = p.simulate(1000)

stability_metric = test_rate_stability(data)
print(f"Rate stability metric: {stability_metric:.3f}")
# Lower values indicate more stable rate 
// Rate stability metric: 0.138
```
